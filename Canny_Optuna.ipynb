{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28103445",
   "metadata": {},
   "source": [
    "### **Модуль детекції вирв та ушкоджень земельних ділянок засобами OpenCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e66f0b",
   "metadata": {},
   "source": [
    "Сучасні технології комп'ютерного зору відкривають нові можливості для автоматизованої детекції кратерів та ушкоджень земельних ділянок. Розробка такої системи є актуальною задачею, особливо в контексті моніторингу територій, постраждалих від військових дій, або для аналізу природних катастроф. Запропонована система детекції поєднує різні методи обробки зображень, від базової підготовки даних до складних алгоритмів розпізнавання образів.\n",
    "\n",
    "#### Функціонально-логічна структура модуля \n",
    "\n",
    "##### 1. Блок підготовки зображень\n",
    "\n",
    "Якість вхідних даних визначає успішність всього процесу детекції, тому етап підготовки зображення є фундаментальним для досягнення точних результатів. Основним інструментом для зменшення шуму при збереженні важливих деталей краю служить двосторонній фільтр cv2.bilateralFilter(). Цей метод особливо ефективний для супутникових знімків, де необхідно зберегти чіткість меж кратерів при одночасному видаленні шуму від атмосферних перешкод або недосконалості сенсорів.\n",
    "\n",
    "Конвертація кольорових зображень у відтінки сірого за допомогою cv2.cvtColor() спрощує подальшу обробку та знижує обчислювальну складність алгоритмів. Покращення контрастності через cv2.filter2D() дозволяє виділити слабко помітні деталі рельєфу, що критично важливо для виявлення дрібних або частково замаскованих пошкоджень.\n",
    "\n",
    "Альтернативні підходи до зменшення шуму включають гауссівське розмиття cv2.GaussianBlur() для загального згладжування та медіанний фільтр cv2.medianBlur() для ефективного видалення імпульсного шуму. Морфологічні операції можуть доповнювати основні методи фільтрації, особливо для усунення дрібних артефактів. При конвертації у відтінки сірого weighted average метод дозволяє налаштувати вагові коефіцієнти для різних кольорових каналів, а luminance-based conversion враховує особливості людського сприйняття яскравості.\n",
    "\n",
    "Для покращення контрастності гістограмна еквалізація cv2.equalizeHist() забезпечує рівномірний розподіл інтенсивності, контрастно-обмежена адаптивна гістограмна еквалізація cv2.createCLAHE() запобігає надмірному підсиленню шуму, а гамма-корекція дозволяє точно налаштувати яскравість у різних діапазонах інтенсивності.\n",
    "\n",
    "##### 2. Блок детекції контурів \n",
    "\n",
    "Виявлення контурів об'єктів становить основу для подальшого розпізнавання кратерів, оскільки більшість пошкоджень земної поверхні характеризуються чіткими межами між пошкодженою та непошкодженою територією. Алгоритм Canny, реалізований через cv2.Canny(), забезпечує оптимальний баланс між точністю детекції та швидкістю обробки. Цей метод використовує двопорогову схему для виділення чітких та розмитих контурів, що дозволяє виявляти як чіткі контури великих кратерів, так і менш виражені межі дрібних ушкоджень.\n",
    "\n",
    "Градієнтний оператор Собеля cv2.Sobel() пропонує альтернативний підхід, особливо ефективний для виявлення країв у певному напрямку. Це корисно при аналізі лінійних пошкоджень або кратерів із характерною орієнтацією. Лапласіан cv2.Laplacian(), як оператор другої похідної, чутливий до швидких змін інтенсивності та може виявляти тонкі деталі контурів, хоча він більш схильний до шуму. Оператор Шарра cv2.Scharr() представляє вдосконалену версію Собеля з кращою ротаційною симетрією, що робить його придатним для детекції країв різної орієнтації.\n",
    "\n",
    "Вибір методу детекції країв залежить від специфіки знімків та типу пошкоджень, які необхідно виявити. Для складних сценаріїв можливе комбінування кількох підходів для досягнення максимальної повноти детекції.\n",
    "\n",
    "##### 2.1. Постобробка результатів детекції із використанням морфологічних операцій\n",
    "\n",
    "Після детекції країв часто виникає необхідність у додатковій обробці для покращення якості контурів та усунення розривів у межах об'єктів. Операція розширення cv2.dilate() відіграє ключову роль у з'єднанні переривчастих країв кратерів, які могли бути пошкоджені шумом або недостатньою роздільною здатністю зображення. Цей метод особливо корисний при роботі із супутниковими знімками низької якості або при наявності часткових затінень.\n",
    "\n",
    "Операція звуження cv2.erode() служить для видалення дрібних артефактів та тонких з'єднань між об'єктами, що помилково можуть бути розпізнані як частини кратерів. Комплексні морфологічні операції cv2.morphologyEx() надають ширші можливості для обробки: операція opening (звуження з подальшим розширенням) ефективно усуває шум при збереженні основних контурів, closing (розширення з подальшим звуженням) заповнює дірки та розриви в об'єктах, а gradient операція виділяє межі об'єктів.\n",
    "\n",
    "Правильний вибір структуруючого елементу та кількості ітерацій морфологічних операцій критично впливає на якість результату. Для кругових об'єктів доцільно використовувати еліптичні або кругові ядра, тоді як для лінійних пошкоджень краще підходять прямокутні структуруючі елементи.\n",
    "\n",
    "##### 2.2. Детекція кратерів із використанням фільтрів для детекції кіл\n",
    "\n",
    "Більшість кратерів та вибухових пошкоджень мають приблизно круглу або еліптичну форму, що робить детекцію кіл центральним елементом системи розпізнавання. Трансформація Хафа для кіл cv2.HoughCircles() є стандартним та найпоширенішим методом для виявлення кругових об'єктів на зображеннях. Алгоритм працює у просторі параметрів, накопичуючи голоси за потенційні центри та радіуси кіл, що дозволяє виявляти навіть частково видимі або спотворені круглі об'єкти.\n",
    "\n",
    "Template matching через cv2.matchTemplate() пропонує альтернативний підхід, особливо ефективний коли відома приблизна форма та розмір кратерів. Цей метод дозволяє шукати специфічні патерни пошкоджень та може бути налаштований для розпізнавання кратерів із характерними особливостями, такими як підвищені краї або центральні заглиблення.\n",
    "\n",
    "Контурний аналіз через cv2.findContours() у поєднанні з апроксимацією еліпсів cv2.fitEllipse() надає гнучкіший підхід до детекції об'єктів неправильної форми. Цей метод дозволяє виявляти не тільки ідеально круглі кратери, але й деформовані або частково зруйновані об'єкти.\n",
    "\n",
    "Сучасні підходи на основі глибокого навчання, такі як YOLO та Faster R-CNN, відкривають нові можливості для високоточної детекції складних об'єктів. Ці методи можуть навчатися на великих датасетах зображень кратерів та автоматично виявляти складні патерни, недоступні для традиційних алгоритмів комп'ютерного зору.\n",
    "\n",
    "##### 3. Географічні трансформації\n",
    "\n",
    "Перетворення координат пікселів у реальні географічні координати є критично важливим для практичного застосування системи детекції. Кожен виявлений кратер повинен бути прив'язаний до конкретної локації на земній поверхні з високою точністю. Процес конвертації враховує рівень масштабування карти та роздільну здатність зображення, оскільки ці параметри безпосередньо впливають на відповідність між пікселями та реальними відстанями.\n",
    "\n",
    "Web Mercator проекція широко використовується у сучасних картографічних сервісах та забезпечує стандартизований спосіб представлення географічних даних. Використання цієї проекції дозволяє легко інтегрувати результати детекції з існуючими геоінформаційними системами та забезпечити сумісність із різними картографічними платформами.\n",
    "\n",
    "Точність географічної прив'язки особливо важлива для координації аварійно-рятувальних робіт або планування відновлювальних заходів. Система повинна враховувати можливі спотворення проекції та забезпечувати коректну калібровку для різних регіонів та масштабів зображень.\n",
    "\n",
    "##### 4. Оптимізація параметрів системи детекції ушкоджень\n",
    "\n",
    "Ефективність системи детекції суттєво залежить від правильного налаштування численних параметрів алгоритмів, що робить автоматизовану оптимізацію критично важливою для досягнення максимальної точності. Інтеграція з бібліотекою Optuna дозволяє систематично досліджувати простір параметрів та знаходити оптимальні налаштування для конкретних типів зображень та умов роботи.\n",
    "\n",
    "Пороги детекції країв у алгоритмі Canny (T1 та T2) визначають чутливість до слабких та сильних країв відповідно. Їх оптимальні значення залежать від контрастності зображень та рівня шуму. Параметри трансформації Хафа для кіл включають роздільну здатність акумулятора dp, мінімальну відстань між центрами кіл min_dist, пороги для детекції країв param1 та центрів кіл param2, а також діапазон радіусів для пошуку.\n",
    "\n",
    "Налаштування двостороннього фільтра впливає на баланс між збереженням деталей та зменшенням шуму, тоді як параметри морфологічних операцій визначають ступінь модифікації контурів. Оптимізація всіх цих параметрів одночасно є складною багатовимірною задачею, яка може бути ефективно вирішена за допомогою сучасних методів автоматичного налаштування гіперпараметрів.\n",
    "\n",
    "Застосування Optuna дозволяє не тільки знайти оптимальні налаштування, але й дослідити взаємозв'язки між різними параметрами та їх вплив на якість детекції. Це забезпечує створення робастної системи, здатної адаптуватися до різних умов та типів вхідних даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b13368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "from geopy import distance\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13604b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ImageMetadata:\n",
    "    \"\"\"Metadata for satellite images\"\"\"\n",
    "    coordinates: Tuple[float, float]  # (lat, lon)\n",
    "    resolution: float  # meters per pixel\n",
    "    zoom_level: int\n",
    "    image_path: str\n",
    "    image_size: Tuple[int, int]  # (width, height)\n",
    "\n",
    "@dataclass\n",
    "class CraterDetection:\n",
    "    \"\"\"Detected crater information\"\"\"\n",
    "    pixel_coords: Tuple[int, int]  # (x, y) in pixels\n",
    "    radius_pixels: int\n",
    "    geo_coords: Tuple[float, float]  # (lat, lon)\n",
    "    radius_meters: float\n",
    "    confidence: float\n",
    "\n",
    "class Metrics(BaseModel):\n",
    "    \"\"\"Evaluation metrics for crater detection\"\"\"\n",
    "    TP: int\n",
    "    FP: int\n",
    "    FN: int\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteImageProcessor:\n",
    "    \"\"\"\n",
    "    Main class for processing satellite images and detecting craters\n",
    "    \n",
    "    Теоретичний опис блоків:\n",
    "    \n",
    "    1. Підготовка зображення:\n",
    "       - Конвертація у відтінки сірого: cv2.cvtColor() - стандартний підхід\n",
    "       - Альтернативи: weighted grayscale, luminance-based conversion\n",
    "       - Покращення контрасту: filter2D з кастомним ядром\n",
    "       - Альтернативи: histogram equalization (equalizeHist), CLAHE, gamma correction\n",
    "       - Фільтрація шуму: bilateralFilter - зберігає краї\n",
    "       - Альтернативи: GaussianBlur, medianBlur, morphological operations\n",
    "    \n",
    "    2. Детекція країв:\n",
    "       - Canny Edge Detection: оптимальний баланс точності та швидкості\n",
    "       - Альтернативи: Sobel, Laplacian, Scharr operators\n",
    "       - Морфологічні операції: dilate для з'єднання переривчастих країв\n",
    "       - Альтернативи: opening, closing, gradient operations\n",
    "    \n",
    "    3. Детекція кіл:\n",
    "       - HoughCircles: стандарт для детекції круглих об'єктів\n",
    "       - Альтернативи: template matching, contour-based detection, \n",
    "         machine learning approaches (YOLO, R-CNN)\n",
    "    \n",
    "    4. Координатні трансформації:\n",
    "       - Pixel to geographic conversion з урахуванням zoom level та resolution\n",
    "       - Використання проекцій (Web Mercator для більшості карт)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_folder: str, output_folder: str = \"output\"):\n",
    "        self.input_folder = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.image_metadata = {}\n",
    "        self.detection_results = {}\n",
    "        self.optimized_params = None\n",
    "        \n",
    "        # Default detection parameters\n",
    "        self.default_params = {\n",
    "            'canny_T1': 91,\n",
    "            'canny_T2': 31,\n",
    "            'dp': 1.20,\n",
    "            'min_dist': 50,\n",
    "            'param1': 50,\n",
    "            'param2': 15,\n",
    "            'min_radius': 5,\n",
    "            'max_radius': 50,\n",
    "            'bilateral_d': 9,\n",
    "            'bilateral_sigma_color': 75,\n",
    "            'bilateral_sigma_space': 75,\n",
    "            'dilate_iterations': 3\n",
    "        }\n",
    "        \n",
    "        # Morphological kernel for dilation\n",
    "        self.dilation_kernel = np.ones((3,3), np.uint8)\n",
    "        \n",
    "        # Create output folder\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Load image metadata\n",
    "        self._load_image_metadata()\n",
    "    \n",
    "    def _load_image_metadata(self):\n",
    "        \"\"\"Load metadata for satellite images from SatelliteImageRetriever\"\"\"\n",
    "        metadata_file = os.path.join(self.input_folder, \"metadata.json\")\n",
    "        \n",
    "        if os.path.exists(metadata_file):\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "                \n",
    "            for img_data in metadata:\n",
    "                img_path = img_data['path']\n",
    "                base_name = Path(img_path).stem\n",
    "                \n",
    "                # Load image to get size\n",
    "                img = cv2.imread(os.path.join(self.input_folder, img_path))\n",
    "                if img is not None:\n",
    "                    height, width = img.shape[:2]\n",
    "                    \n",
    "                    self.image_metadata[base_name] = ImageMetadata(\n",
    "                        coordinates=(img_data['lat'], img_data['lon']),\n",
    "                        resolution=img_data['resolution'],\n",
    "                        zoom_level=img_data['zoom'],\n",
    "                        image_path=img_path,\n",
    "                        image_size=(width, height)\n",
    "                    )\n",
    "        else:\n",
    "            logger.warning(f\"Metadata file not found: {metadata_file}\")\n",
    "            # Fallback: scan folder and create dummy metadata\n",
    "            self._create_dummy_metadata()\n",
    "    \n",
    "    def _create_dummy_metadata(self):\n",
    "        \"\"\"Create dummy metadata when metadata.json is not available\"\"\"\n",
    "        image_files = glob.glob(os.path.join(self.input_folder, \"*.jpg\")) + \\\n",
    "                     glob.glob(os.path.join(self.input_folder, \"*.png\"))\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            base_name = Path(img_path).stem\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                height, width = img.shape[:2]\n",
    "                \n",
    "                self.image_metadata[base_name] = ImageMetadata(\n",
    "                    coordinates=(0.0, 0.0),  # Default coordinates\n",
    "                    resolution=1.0,  # Default resolution\n",
    "                    zoom_level=15,   # Default zoom\n",
    "                    image_path=os.path.basename(img_path),\n",
    "                    image_size=(width, height)\n",
    "                )\n",
    "    \n",
    "    def contrast_enhancement(self, img: np.ndarray, kernel: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Enhance image contrast using convolution\n",
    "        \n",
    "        Альтернативи:\n",
    "        - CLAHE: cv2.createCLAHE()\n",
    "        - Histogram equalization: cv2.equalizeHist()\n",
    "        - Gamma correction: np.power(img/255.0, gamma) * 255\n",
    "        \"\"\"\n",
    "        if kernel is None:\n",
    "            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        \n",
    "        return cv2.filter2D(img, -1, kernel)\n",
    "    \n",
    "    def preprocess_image(self, img: np.ndarray, params: Dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Complete image preprocessing pipeline\n",
    "        \n",
    "        Кроки:\n",
    "        1. Bilateral filtering - шумо зниження зі збереженням країв\n",
    "        2. Grayscale conversion\n",
    "        3. Contrast enhancement\n",
    "        \"\"\"\n",
    "        # Apply bilateral filter to smooth the image while preserving edges\n",
    "        img_filtered = cv2.bilateralFilter(\n",
    "            img, \n",
    "            params['bilateral_d'], \n",
    "            params['bilateral_sigma_color'], \n",
    "            params['bilateral_sigma_space']\n",
    "        )\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img_filtered, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Enhance contrast\n",
    "        contrast = self.contrast_enhancement(gray)\n",
    "        \n",
    "        return contrast\n",
    "    \n",
    "    def detect_edges(self, img: np.ndarray, params: Dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Edge detection using Canny algorithm\n",
    "        \n",
    "        Альтернативи:\n",
    "        - Sobel: cv2.Sobel()\n",
    "        - Laplacian: cv2.Laplacian()\n",
    "        - Scharr: cv2.Scharr()\n",
    "        \"\"\"\n",
    "        canny = cv2.Canny(img, params['canny_T1'], params['canny_T2'])\n",
    "        \n",
    "        # Dilate edges to connect broken edge segments\n",
    "        dilated = cv2.dilate(canny, self.dilation_kernel, iterations=params['dilate_iterations'])\n",
    "        \n",
    "        return dilated\n",
    "    \n",
    "    def detect_circles(self, img: np.ndarray, params: Dict) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Detect circles using Hough Circle Transform\n",
    "        \n",
    "        Альтернативи:\n",
    "        - Template matching: cv2.matchTemplate()\n",
    "        - Contour-based detection: cv2.findContours() + cv2.fitEllipse()\n",
    "        - Deep learning approaches (YOLO, Faster R-CNN)\n",
    "        \"\"\"\n",
    "        circles = cv2.HoughCircles(\n",
    "            img, \n",
    "            cv2.HOUGH_GRADIENT, \n",
    "            dp=params['dp'],\n",
    "            minDist=params['min_dist'],\n",
    "            param1=params['param1'], \n",
    "            param2=params['param2'],\n",
    "            minRadius=params['min_radius'], \n",
    "            maxRadius=params['max_radius']\n",
    "        )\n",
    "        \n",
    "        return circles\n",
    "    \n",
    "    def circles_to_bbox(self, circles: np.ndarray, xywh: bool = True) -> List[List[float]]:\n",
    "        \"\"\"Convert circles to bounding boxes\"\"\"\n",
    "        bboxes = []\n",
    "        for circle in circles:\n",
    "            if xywh:\n",
    "                x_c, y_c, w, h = circle[0], circle[1], circle[2]*2, circle[2]*2\n",
    "                bboxes.append([x_c, y_c, w, h])\n",
    "            else:\n",
    "                x1, y1, x2, y2 = (circle[0] - circle[2],\n",
    "                                  circle[1] - circle[2], \n",
    "                                  circle[0] + circle[2],\n",
    "                                  circle[1] + circle[2])\n",
    "                bboxes.append([x1, y1, x2, y2])\n",
    "        return bboxes\n",
    "    \n",
    "    def pixel_to_geo_coords(self, pixel_coords: Tuple[int, int], \n",
    "                           image_metadata: ImageMetadata) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Convert pixel coordinates to geographic coordinates\n",
    "        \n",
    "        Використовує Web Mercator проекцію (EPSG:3857)\n",
    "        \"\"\"\n",
    "        x_pixel, y_pixel = pixel_coords\n",
    "        width, height = image_metadata.image_size\n",
    "        center_lat, center_lon = image_metadata.coordinates\n",
    "        resolution = image_metadata.resolution\n",
    "        \n",
    "        # Calculate offset from image center in meters\n",
    "        x_offset_pixels = x_pixel - width / 2\n",
    "        y_offset_pixels = height / 2 - y_pixel  # Flip Y axis\n",
    "        \n",
    "        x_offset_meters = x_offset_pixels * resolution\n",
    "        y_offset_meters = y_offset_pixels * resolution\n",
    "        \n",
    "        # Use geopy for accurate distance calculations\n",
    "        from geopy import distance\n",
    "        from geopy.distance import distance as geopy_distance\n",
    "        \n",
    "        # Calculate new coordinates\n",
    "        start_point = (center_lat, center_lon)\n",
    "        \n",
    "        # Move east/west\n",
    "        if x_offset_meters != 0:\n",
    "            bearing_x = 90 if x_offset_meters > 0 else 270\n",
    "            new_point = geopy_distance(meters=abs(x_offset_meters)).destination(\n",
    "                start_point, bearing_x\n",
    "            )\n",
    "            center_lat, center_lon = new_point.latitude, new_point.longitude\n",
    "        \n",
    "        # Move north/south\n",
    "        if y_offset_meters != 0:\n",
    "            bearing_y = 0 if y_offset_meters > 0 else 180\n",
    "            final_point = geopy_distance(meters=abs(y_offset_meters)).destination(\n",
    "                (center_lat, center_lon), bearing_y\n",
    "            )\n",
    "            return final_point.latitude, final_point.longitude\n",
    "        \n",
    "        return center_lat, center_lon\n",
    "    \n",
    "    def pixels_to_meters(self, radius_pixels: int, resolution: float) -> float:\n",
    "        \"\"\"Convert pixel radius to meters\"\"\"\n",
    "        return radius_pixels * resolution\n",
    "    \n",
    "    def detect_craters_single_image(self, image_name: str, \n",
    "                                  params: Dict = None) -> List[CraterDetection]:\n",
    "        \"\"\"Detect craters in a single image\"\"\"\n",
    "        if params is None:\n",
    "            params = self.default_params\n",
    "        \n",
    "        if image_name not in self.image_metadata:\n",
    "            logger.error(f\"No metadata found for image: {image_name}\")\n",
    "            return []\n",
    "        \n",
    "        metadata = self.image_metadata[image_name]\n",
    "        img_path = os.path.join(self.input_folder, metadata.image_path)\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            logger.error(f\"Could not load image: {img_path}\")\n",
    "            return []\n",
    "        \n",
    "        # Preprocess image\n",
    "        processed = self.preprocess_image(img, params)\n",
    "        \n",
    "        # Detect edges\n",
    "        edges = self.detect_edges(processed, params)\n",
    "        \n",
    "        # Detect circles\n",
    "        circles = self.detect_circles(edges, params)\n",
    "        \n",
    "        detections = []\n",
    "        if circles is not None:\n",
    "            circles = np.uint16(np.around(circles[0]))\n",
    "            \n",
    "            for circle in circles:\n",
    "                x, y, r = circle\n",
    "                \n",
    "                # Convert to geographic coordinates\n",
    "                geo_coords = self.pixel_to_geo_coords((x, y), metadata)\n",
    "                radius_meters = self.pixels_to_meters(r, metadata.resolution)\n",
    "                \n",
    "                detection = CraterDetection(\n",
    "                    pixel_coords=(x, y),\n",
    "                    radius_pixels=r,\n",
    "                    geo_coords=geo_coords,\n",
    "                    radius_meters=radius_meters,\n",
    "                    confidence=0.8  # Default confidence\n",
    "                )\n",
    "                detections.append(detection)\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def detect_all_craters(self, params: Dict = None) -> Dict[str, List[CraterDetection]]:\n",
    "        \"\"\"Detect craters in all images\"\"\"\n",
    "        if params is None:\n",
    "            params = self.default_params\n",
    "        \n",
    "        results = {}\n",
    "        for image_name in self.image_metadata.keys():\n",
    "            logger.info(f\"Processing image: {image_name}\")\n",
    "            detections = self.detect_craters_single_image(image_name, params)\n",
    "            results[image_name] = detections\n",
    "            logger.info(f\"Found {len(detections)} craters in {image_name}\")\n",
    "        \n",
    "        self.detection_results = results\n",
    "        return results\n",
    "    \n",
    "    def save_results(self, filename: str = \"crater_detections.json\"):\n",
    "        \"\"\"Save detection results to JSON file\"\"\"\n",
    "        output_path = os.path.join(self.output_folder, filename)\n",
    "        \n",
    "        # Convert results to serializable format\n",
    "        serializable_results = {}\n",
    "        for image_name, detections in self.detection_results.items():\n",
    "            serializable_results[image_name] = []\n",
    "            for detection in detections:\n",
    "                serializable_results[image_name].append({\n",
    "                    'pixel_coords': detection.pixel_coords,\n",
    "                    'radius_pixels': detection.radius_pixels,\n",
    "                    'geo_coords': detection.geo_coords,\n",
    "                    'radius_meters': detection.radius_meters,\n",
    "                    'confidence': detection.confidence\n",
    "                })\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(serializable_results, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Results saved to: {output_path}\")\n",
    "    \n",
    "    def save_metadata(self, filename: str = \"detection_metadata.json\"):\n",
    "        \"\"\"Save metadata about detection process\"\"\"\n",
    "        output_path = os.path.join(self.output_folder, filename)\n",
    "        \n",
    "        metadata = {\n",
    "            'input_folder': self.input_folder,\n",
    "            'output_folder': self.output_folder,\n",
    "            'total_images': len(self.image_metadata),\n",
    "            'total_detections': sum(len(detections) for detections in self.detection_results.values()),\n",
    "            'parameters_used': self.optimized_params if self.optimized_params else self.default_params,\n",
    "            'images_metadata': {}\n",
    "        }\n",
    "        \n",
    "        for name, img_meta in self.image_metadata.items():\n",
    "            metadata['images_metadata'][name] = {\n",
    "                'coordinates': img_meta.coordinates,\n",
    "                'resolution': img_meta.resolution,\n",
    "                'zoom_level': img_meta.zoom_level,\n",
    "                'image_size': img_meta.image_size,\n",
    "                'detections_count': len(self.detection_results.get(name, []))\n",
    "            }\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Metadata saved to: {output_path}\")\n",
    "    \n",
    "    def visualize_detections(self, image_name: str, save_image: bool = True):\n",
    "        \"\"\"Visualize detected craters on image\"\"\"\n",
    "        if image_name not in self.image_metadata:\n",
    "            logger.error(f\"No metadata found for image: {image_name}\")\n",
    "            return\n",
    "        \n",
    "        metadata = self.image_metadata[image_name]\n",
    "        img_path = os.path.join(self.input_folder, metadata.image_path)\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            logger.error(f\"Could not load image: {img_path}\")\n",
    "            return\n",
    "        \n",
    "        # Draw detections\n",
    "        detections = self.detection_results.get(image_name, [])\n",
    "        vis_img = img.copy()\n",
    "        \n",
    "        for detection in detections:\n",
    "            x, y = detection.pixel_coords\n",
    "            r = detection.radius_pixels\n",
    "            \n",
    "            # Draw circle\n",
    "            cv2.circle(vis_img, (x, y), r, (0, 255, 0), 2)\n",
    "            # Draw center\n",
    "            cv2.circle(vis_img, (x, y), 2, (0, 0, 255), 3)\n",
    "            # Add text with radius\n",
    "            cv2.putText(vis_img, f'r={detection.radius_meters:.1f}m', \n",
    "                       (x-20, y-r-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        if save_image:\n",
    "            output_path = os.path.join(self.output_folder, f\"{image_name}_detections.jpg\")\n",
    "            cv2.imwrite(output_path, vis_img)\n",
    "            logger.info(f\"Visualization saved to: {output_path}\")\n",
    "        \n",
    "        return vis_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class OptunaOptimizer:\n",
    "    \"\"\"Optuna-based parameter optimization for crater detection\"\"\"\n",
    "    \n",
    "    def __init__(self, processor: SatelliteImageProcessor, ground_truth_dir: str = None):\n",
    "        self.processor = processor\n",
    "        self.ground_truth_dir = ground_truth_dir\n",
    "    \n",
    "    def calculate_iou(self, boxA: List[float], boxB: List[float]) -> float:\n",
    "        \"\"\"Calculate Intersection over Union (IoU) for two bounding boxes\"\"\"\n",
    "        x1A, y1A, x2A, y2A = boxA\n",
    "        x1B, y1B, x2B, y2B = boxB\n",
    "        \n",
    "        x1_intersection = max(x1A, x1B)\n",
    "        y1_intersection = max(y1A, y1B)\n",
    "        x2_intersection = min(x2A, x2B)\n",
    "        y2_intersection = min(y2A, y2B)\n",
    "        \n",
    "        intersection_area = max(0, x2_intersection - x1_intersection) * \\\n",
    "                          max(0, y2_intersection - y1_intersection)\n",
    "        \n",
    "        boxA_area = (x2A - x1A) * (y2A - y1A)\n",
    "        boxB_area = (x2B - x1B) * (y2B - y1B)\n",
    "        union_area = boxA_area + boxB_area - intersection_area\n",
    "        \n",
    "        return intersection_area / union_area if union_area > 0 else 0\n",
    "    \n",
    "    def calculate_metrics(self, predictions: List[List[float]], \n",
    "                         ground_truths: List[List[float]], \n",
    "                         iou_threshold: float = 0.5) -> Metrics:\n",
    "        \"\"\"Calculate precision, recall, and F1 score\"\"\"\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        detected = []\n",
    "        \n",
    "        for pred in predictions:\n",
    "            found_match = False\n",
    "            for truth in ground_truths:\n",
    "                iou = self.calculate_iou(pred, truth)\n",
    "                if iou >= iou_threshold and truth not in detected:\n",
    "                    TP += 1\n",
    "                    detected.append(truth)\n",
    "                    found_match = True\n",
    "                    break\n",
    "            if not found_match:\n",
    "                FP += 1\n",
    "        \n",
    "        FN = len(ground_truths) - len(detected)\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return Metrics(\n",
    "            TP=TP, FP=FP, FN=FN,\n",
    "            precision=precision, recall=recall, f1_score=f1_score\n",
    "        )\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"Optuna objective function\"\"\"\n",
    "        # Define parameter search space\n",
    "        params = {\n",
    "            'canny_T1': trial.suggest_int('canny_T1', 50, 150),\n",
    "            'canny_T2': trial.suggest_int('canny_T2', 20, 100),\n",
    "            'dp': trial.suggest_float('dp', 1.0, 2.0),\n",
    "            'min_dist': trial.suggest_int('min_dist', 20, 100),\n",
    "            'param1': trial.suggest_int('param1', 30, 100),\n",
    "            'param2': trial.suggest_int('param2', 10, 50),\n",
    "            'min_radius': trial.suggest_int('min_radius', 3, 20),\n",
    "            'max_radius': trial.suggest_int('max_radius', 15, 100),\n",
    "            'bilateral_d': trial.suggest_int('bilateral_d', 5, 15),\n",
    "            'bilateral_sigma_color': trial.suggest_int('bilateral_sigma_color', 50, 150),\n",
    "            'bilateral_sigma_space': trial.suggest_int('bilateral_sigma_space', 50, 150),\n",
    "            'dilate_iterations': trial.suggest_int('dilate_iterations', 1, 5)\n",
    "        }\n",
    "        \n",
    "        # Run detection with current parameters\n",
    "        results = self.processor.detect_all_craters(params)\n",
    "        \n",
    "        if not self.ground_truth_dir:\n",
    "            # If no ground truth, use detection count as proxy metric\n",
    "            total_detections = sum(len(detections) for detections in results.values())\n",
    "            return total_detections\n",
    "        \n",
    "        # Calculate metrics against ground truth\n",
    "        total_metrics = Metrics(TP=0, FP=0, FN=0, precision=0, recall=0, f1_score=0)\n",
    "        \n",
    "        for image_name, detections in results.items():\n",
    "            # Convert detections to bounding boxes\n",
    "            pred_bboxes = []\n",
    "            for det in detections:\n",
    "                x, y, r = det.pixel_coords[0], det.pixel_coords[1], det.radius_pixels\n",
    "                pred_bboxes.append([x-r, y-r, x+r, y+r])\n",
    "            \n",
    "            # Load ground truth if available\n",
    "            gt_path = os.path.join(self.ground_truth_dir, f\"{image_name}.txt\")\n",
    "            if os.path.exists(gt_path):\n",
    "                with open(gt_path, 'r') as f:\n",
    "                    gt_lines = f.readlines()\n",
    "                \n",
    "                gt_bboxes = []\n",
    "                for line in gt_lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        # Assuming YOLO format: class x_center y_center width height\n",
    "                        _, x_c, y_c, w, h = map(float, parts)\n",
    "                        img_w, img_h = self.processor.image_metadata[image_name].image_size\n",
    "                        x_c *= img_w\n",
    "                        y_c *= img_h\n",
    "                        w *= img_w\n",
    "                        h *= img_h\n",
    "                        gt_bboxes.append([x_c - w/2, y_c - h/2, x_c + w/2, y_c + h/2])\n",
    "                \n",
    "                metrics = self.calculate_metrics(pred_bboxes, gt_bboxes)\n",
    "                total_metrics.TP += metrics.TP\n",
    "                total_metrics.FP += metrics.FP\n",
    "                total_metrics.FN += metrics.FN\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        total_metrics.precision = total_metrics.TP / (total_metrics.TP + total_metrics.FP) \\\n",
    "                                 if (total_metrics.TP + total_metrics.FP) > 0 else 0\n",
    "        total_metrics.recall = total_metrics.TP / (total_metrics.TP + total_metrics.FN) \\\n",
    "                              if (total_metrics.TP + total_metrics.FN) > 0 else 0\n",
    "        total_metrics.f1_score = 2 * total_metrics.precision * total_metrics.recall / \\\n",
    "                                (total_metrics.precision + total_metrics.recall) \\\n",
    "                                if (total_metrics.precision + total_metrics.recall) > 0 else 0\n",
    "        \n",
    "        return total_metrics.f1_score\n",
    "    \n",
    "    def optimize(self, n_trials: int = 100) -> Dict:\n",
    "        \"\"\"Run optimization\"\"\"\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self.objective, n_trials=n_trials)\n",
    "        \n",
    "        logger.info(\"Optimization completed!\")\n",
    "        logger.info(f\"Best parameters: {study.best_params}\")\n",
    "        logger.info(f\"Best value: {study.best_value}\")\n",
    "        \n",
    "        # Update processor with optimized parameters\n",
    "        optimized_params = self.processor.default_params.copy()\n",
    "        optimized_params.update(study.best_params)\n",
    "        self.processor.optimized_params = optimized_params\n",
    "        \n",
    "        return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efee6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to demonstrate usage\"\"\"\n",
    "    # Initialize processor\n",
    "    input_folder = \"satellite_images\"  # Folder with images from SatelliteImageRetriever\n",
    "    processor = SatelliteImageProcessor(input_folder)\n",
    "    \n",
    "    # Optional: Optimize parameters\n",
    "    optimizer = OptunaOptimizer(processor)\n",
    "    # best_params = optimizer.optimize(n_trials=50)\n",
    "    \n",
    "    # Detect craters\n",
    "    results = processor.detect_all_craters()\n",
    "    \n",
    "    # Save results\n",
    "    processor.save_results()\n",
    "    processor.save_metadata()\n",
    "    \n",
    "    # Visualize results for each image\n",
    "    for image_name in processor.image_metadata.keys():\n",
    "        processor.visualize_detections(image_name)\n",
    "    \n",
    "    logger.info(\"Crater detection completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
